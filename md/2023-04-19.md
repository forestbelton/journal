Today I learned about perceptual hashing techniques. I thought it would be interesting to see if I could devise a method to use builtin SQLite index types to construct a spatial index over perceptual hashes.

I skimmed through a survey paper [State of the Art: Image Hashing](https://arxiv.org/abs/2108.11794) that described some of the approaches out there. Performance differences between RP-IVD, SS-Salient-SF, pHash, and F-DNS were explored. Models were tested for their resilience against certain content-preserving operations. Of these, pHash seemed like the most worth exploring at this time.

I also noted that this paper was written in 2021 and it's possible the field has also rapidly progressed since then. In particular, the results in [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) looked promising, but I haven't read it yet. It would be interesting to know if an image recognizer could be finetuned from a trained LLM.

### Locating a mistake

I located the [pHash website](https://www.phash.org/) and read the relevant sections in the only publication listed: [Implementation and Benchmarking of Perceptual Image Hash Functions](https://www.phash.org/docs/pubs/thesis_zauner.pdf). I was previously unfamiliar with [discrete cosine transforms](https://en.wikipedia.org/wiki/Discrete_cosine_transform) but there wasn't much left to understand after learning that they are simply a special case of the discrete Fourier transform. There is an error in the leading factor of equation 3.3, which describes elements in the DCT matrix of size $N$:
$$c\left[n, m\right] = \sqrt{\frac{2}{N}} \cos{\frac{\left(2m+1\right) n \pi}{2N}}$$
The DCT matrix fails to be orthonormal under this definition. The correct equation can be found as 3.13 in the [chapter 3 course notes](https://ocw.mit.edu/courses/6-050j-information-and-entropy-spring-2008/9f67d1d414e446c9b55925ab92c17c15_MIT6_050JS08_chapter3.pdf) for the course [6.050J Information and Entropy](https://ocw.mit.edu/courses/6-050j-information-and-entropy-spring-2008/) on MIT OpenCourseWare from Spring 2008, where the leading factor has been changed to $\sqrt{\frac{1}{N}}$ for the first row:
$$c\left[n, m\right] = \sqrt{\frac{k_n}{N}} \cos{\frac{\left(2m+1\right) n \pi}{2N}}$$
Where $k_n$ is given by:
$$k_n = \begin{cases}
\sqrt{\frac{1}{N}} & n = 0 \\
\sqrt{\frac{2}{N}} & \text{otherwise}
\end{cases}$$

## DCT perceptual hash algorithm

After deciphering the issue, I finished reading the thesis' description of the algorithm. Given an input image, computing its DCT perceptual hash proceeds as follows:

1. Convert the image to grayscale (0-255) using only its luminance.
2. Apply a 7x7 [box blur](https://en.wikipedia.org/wiki/Box_blur) to the image using an [image convolution](). Warping will occur along the edges unless padding is added or the image is clipped afterwards. Since we are about to resize, it is probably fine to just clip.
3. Resize the image to 32x32.
4. Compute $\text{DCT}\left(I\right) = M \, I \, M^T$, where $I$ is the image and $M$ is the DCT matrix of size 32. Since the image size is now constant, $M$ and $M^T$ can be precomputed.
5. Take 64 low-frequency components (omitting lowest) DCT[1:8, 1:8] and flatten into a 1-dimensional sequence. Doesn't matter how the flatten happens, just be consistent.
6. Calculate the median $m$ of the sequence and map each element to 0 if it is less than $m$, or 1 otherwise. We now have a bit string of length 64.
7. Convert to a 64-bit unsigned integer.

I wrote a simple implementation of this using numpy and scipy. Originally I had planned to use numpy only, but in order to easily apply an image convolution I needed `scipy.ndimage.convolve`. I only detected the bug in the paper while writing unit tests.

## SQLite

Now that I had a method to generate perceptual hashes, I was interested in efficiently matching (via Hamming distance) an input hash against a database of known hashes. I surveyed some SQLite extensions for suitable index types. [k-d trees](https://en.wikipedia.org/wiki/K-d_tree) seemed like an interesting choice but they don't scale well into high (e.g. 64) dimensions.

### R\*Trees

The first thought I had was to use an [R\*Tree](https://dl.acm.org/doi/pdf/10.1145/971697.602266) across 64 dimensions. However, I was disappointed to learn that SQLite's [R\*Tree extension](https://www.sqlite.org/rtree.html) only supports anything wider than 5 dimensions. This still seemed like an interesting theoretical avenue, so I spent some time exploring the idea.

Since the Hamming distance generates the taxicab metric over perceptual hashes embedded in $\mathbb{R}^n$, I found that even though R\*Trees are assumed to be across the Euclidean metric, I could use a quasi-isometry to map perceptual hashes to real numbers. The quasi-isometry preserves distances by a constant multiple, which seems sufficient to retain the usefulness of R\*Trees. This seems like a generally useful method that works for any metric space with a quasi-isometry to $\mathbb{R}^n$.

### spellfix1

The [spellfix1](https://www.sqlite.org/spellfix1.html) virtual table extension includes a spellchecker that can be utilized for searching for hashes with close Hamming distances, e.g.:

```sql
CREATE VIRTUAL TABLE perceptual_hashes
USING spellfix1;

INSERT INTO perceptual_hashes (word)
VALUES
  ('0011110111110001001101111010011110011001000010000101100001001101'),
  ('1010110100011111001100010001111000101000111101101010000100110000'),
  -- ...
  ('1010000010000101001010000011001111011001001101011101100001000110');

SELECT word AS hash
FROM perceptual_hashes
WHERE word MATCH '0011001001100110111010100010001110110011101000011100101100011011';
```

### Decision

I haven't had the chance to evaluate these methods in practice. spellfix1 has a straightforward implementation so it seems the most feasible. Through clever encoding it may be possible to use R\*Trees or increase the performance of spellfix1 itself.